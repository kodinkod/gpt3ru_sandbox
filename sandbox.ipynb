{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбираемся с тем, как можно взаимодействовать с GPT3 от сбера "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sberbank-ai/rugpt3large_based_on_gpt2\" # прописываем имя до модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Александр Сергеевич Пушкин родился в \n",
      "1799 году в Москве.\n",
      "\n",
      "В\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "text = \"Александр Сергеевич Пушкин родился в \"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаём  входные параметры/ эмбеддинг"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дописывание текста\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"2+2 = \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  22,   15,   22, 8484,  225]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(input_ids, max_new_tokens=10, eos_token_id=50256, pad_token_id=50256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2+2 = \\n4.\\n\\n4.\\n\\n5']\n",
      "2+2 = \n",
      "4.\n",
      "\n",
      "4.\n",
      "\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(list(map(tokenizer.decode, out)))\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суммаризация\n",
    "----\n",
    "\n",
    "Если в конце текста добавлтяь TL;DR, то нейросеть начинает генерировать краткое содержание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "Когда я оплачиваю покупку в магазине, я вижу текст без засечек на экране терминала. У цветочных магазинов — вывески в виде прописного шрифта, как будто их кто то написал пером. Почему так ? Почему цветочные магазины не пишут вывески строгим или жирным шрифтом?\n",
    "TL;DR\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\" \n",
    "В детстве ненавидел художественную литературу. Все книги в которых было хоть что-то с малейшим намеком на сюжет с главными героями — шли мимо. Особенно это касается русской литературы в которой сначала ты читаешь сказки про жадную старуху или хитрую лису и не очень хитрого колобка, а к 10 классу ты уже разбираешь депрессивные книги в которых кто-то да точно умрет.TL;DR\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Когда я оплачиваю покупку в магазине, я вижу текст без засечек на экране терминала. У цветочных магазинов — вывески в виде прописного шрифта, как будто их кто то написал пером. Почему так? Почему цветочные магазины не пишут вывески строгим или жирным шрифтом?\n",
      "TL;DR\n",
      "\n",
      "В общем, я не знаю, что делать.\n",
      "\n",
      "UPD:\n",
      "\n",
      "В ответ на комментарий от :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "out = model.generate(input_ids, max_new_tokens=25, eos_token_id=50256, pad_token_id=50256)\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем на короткрих текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Я не люблю есть красные пельмени, потому что белые пельмени вкусней красных. Об этом мне рассказала Настя. TL;DR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я не люблю есть красные пельмени, потому что белые пельмени вкусней красных. Об этом мне рассказала Настя. TL;DR;\n",
      "\n",
      "- Я не люблю есть пельмени, потому что они не вкусные. Об этом мне рассказала Настя. T\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "out = model.generate(input_ids, max_new_tokens=25, eos_token_id=50256, pad_token_id=50256)\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "умеет даже так:\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "По-русски: 'кот', по-английски: 'кот').\n",
      "\n",
      "В этом случае, как и в случае с \"котятами\", \"котята\" - это не только котята\n"
     ]
    }
   ],
   "source": [
    "text = \"По-русски: 'кот', по-английски:\" \n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "out = model.generate(input_ids, max_new_tokens=30, do_sample=False)\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "С Настей я познакомился в октябре 2020, мы сильно подружились, а еще через пол года я начал признавться ей в любви - дурак. TL;DR;\n",
      "\n",
      "- я не могу без нее жить, она моя жизнь, я не могу без нее жить, она моя жизнь\n"
     ]
    }
   ],
   "source": [
    "text = \"С Настей я познакомился в октябре 2020, мы сильно подружились, а еще через пол года я начал признавться ей в любви - дурак. TL;DR\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "out = model.generate(input_ids, max_new_tokens=25, eos_token_id=50256, pad_token_id=50256)\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация \n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Токенизируй меня\n",
      "tokens:  [789, 368, 337, 848, 28306, 703]\n",
      "decoded tokens:  ['Т', 'ок', 'ени', 'зи', 'руй', ' меня']\n"
     ]
    }
   ],
   "source": [
    "# Изначальные текст\n",
    "text = \"Токенизируй меня\" \n",
    "# Процесс токенизации с помощьюю токенайзера ruGPT-3\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False) \n",
    "# Обратная поэлементая токенизация\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in tokens] \n",
    "\n",
    "print(\"text:\", text)\n",
    "print(\"tokens: \", tokens)\n",
    "print(\"decoded tokens: \", decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�\n",
      "�\n",
      "�\n",
      "撝\n"
     ]
    }
   ],
   "source": [
    "# Эти три токена по отдельности не декодируются\n",
    "print(tokenizer.decode([167]))\n",
    "#>>> �\n",
    "print(tokenizer.decode([245]))\n",
    "#>>> �\n",
    "print(tokenizer.decode([256]))\n",
    "#>>> �\n",
    "\n",
    "\n",
    "# Но вместе они образуют иероглиф\n",
    "print(tokenizer.decode([167, 245, 256]))\n",
    "#>>> 撝"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы генерации текста\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Определение: \"Нейронная сеть\" - это'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy Search\n",
    "---\n",
    "Самый простой способ — это аргмаксная генерация (greedy search), когда мы каждый раз выбираем токен, у которого максимальная вероятность. Это не самый лучший метод, так как в таком случае генерация застревает в локальных минимумах и часто выдаёт повторяющиеся фрагменты, например the the the the ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Определение: \"Нейронная сеть\" - это компьютерная программа, которая позволяет создавать и анализировать нейронные сети. Нейронные сети\n"
     ]
    }
   ],
   "source": [
    "# Пример аргмаксного сэмплирования\n",
    "out = model.generate(input_ids, \n",
    "                     do_sample=False, \n",
    "                     max_length=30)\n",
    "\n",
    "# Декодирование токенов\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search\n",
    "--------\n",
    "\n",
    "Чуть более сложный и качественный способ сэмплирования — это beam search. В этом случае на каждом шаге мы выбираем не только один самый вероятный токен, а сразу несколько (beam-size), и дальше продолжаем поиск для каждого из выбранных токенов. Таким образом мы разветвляем пути генерации, получая несколько вариантов сгенерированного текста. В итоге можно выбрать тот вариант, у которого лучшая перплексити (уверенность модели в реалистичности текста). Такая генерация обладает хорошей когерентностью (связностью), но обычно у них не хватает \"человечности\", они кажутся сухими и скучными. Также это не полностью решает проблему повторяющихся фрагментов генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Определение: \"Нейронная сеть\" - это сеть, состоящая из множества нейронов, соединенных друг с другом. Нейронная\n"
     ]
    }
   ],
   "source": [
    "# Пример генерации с помощью beam-search\n",
    "out = model.generate(input_ids, \n",
    "                     do_sample=False, \n",
    "                     num_beams=5, \n",
    "                     max_length=30)\n",
    "\n",
    "# Декодирование токенов\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Я долго думал и в конце захотел создать свой супер интересный блог в котором буду рассказывать новости TL;DR'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я долго думал и в конце захотел создать свой супер интересный блог в котором буду рассказывать новости TL;DR;DR;DR;DR;DR;\n"
     ]
    }
   ],
   "source": [
    "# Пример генерации с помощью beam-search\n",
    "out = model.generate(input_ids, \n",
    "                     do_sample=False, \n",
    "                     num_beams=5, \n",
    "                     max_length=30)\n",
    "\n",
    "# Декодирование токенов\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея - совместить два метода генерации текста \n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сэмплирование с температурой\n",
    "---\n",
    "\n",
    "Чтобы добавить тексту непредсказуемости и человечности можно использовать вероятностное сэмплирование с температурой. При такой генерации мы берём не самый вероятный токен, а выбираем его \"случайно\" с учётом распределения вероятностей. Параметр температуры позволяет контролировать степень рандомности. При нулевой температуре этот метод совпадает с аргмаксным сэмплированием, а при очень большой температуре токены будут выбираться совсем наугад. Обычно хорошо работает температура в диапазоне 0.8 - 2.0. Формула модификации распределения вероятностей очень похожа на формулу распределения Больцмана. Чем выше температура системы тем больше \"размазывается\" распределение вероятностей её возможных состояний, отсюда слово \"температура\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Любовь это как еда и как музыка)) )<s>\n",
      "Любовь женщины не знает денег - её не обманует ничего - да?\n",
      "у всех бывает,\n"
     ]
    }
   ],
   "source": [
    "text = 'Любовь это'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Пример вероятностного сэмплирования\n",
    "out = model.generate(input_ids, \n",
    "                     do_sample=True, \n",
    "                     temperature=1.9,  # регулируем темпиратуру\n",
    "                     max_length=30)\n",
    "\n",
    "# Декодирование токенов\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сэмплирование с ограничением маловероятных токенов (Nucleus Sampling)\n",
    "---\n",
    "\n",
    "Для запрета сэмплирования совсем уж некорректных токенов вводят top-k или top-p ограничения. В этом случае генерация тоже происходит случайным образом, но мы заранее отсекаем все маловероятные токены. В случае top-k мы просто зануляем все вероятности кроме k самых больших. А в случае top-p мы оставляем такой минимальный сет токенов, чтобы сумма их вероятностей была не больше p.  Ограничение top-p иногда называют nucleus sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Любовь это все!\n",
      "\n",
      "Как-то вечером на кухне в доме, принадлежавшем, судя по всему, богатой семье, мы сидели в большой столовой\n"
     ]
    }
   ],
   "source": [
    "# Пример вероятностного сэмплирвоания с ограничением\n",
    "out = model.generate(input_ids, \n",
    "                     do_sample=True,\n",
    "                     temperature=1.3,\n",
    "                     top_k=20,\n",
    "                     top_p=0.8,\n",
    "                     max_length=30,\n",
    "                    )\n",
    "# Декодирование токенов\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пробуем суммаризацию\n",
    "text = \"\"\" В детстве ненавидел художественную литературу. Все книги в которых было хоть что-то с малейшим намеком на сюжет с главными героями — шли мимо. Особенно это касается русской литературы в которой сначала ты читаешь сказки про жадную старуху или хитрую лису и не очень хитрого колобка, а к 10 классу ты уже разбираешь депрессивные книги в которых кто-то да точно умрет.\n",
    "\n",
    "Это не значит, что я ненавидел вообще книги, просто я читал другие. Мне нравился научный сленг и естественно я зачитывался детскими энциклопедиями. Такими чтоб картинок побольше было и текст поинтересней и обязательно про космоc. Планеты, полёты в космос, черные дыры это лайк. В то время я хотел стать астрономом.\n",
    "\n",
    "Что касается книг про динозавров для меня это было ерундой, пусть динозавры сами про себя и читают, они не имеют ничего общего с настоящим миром.\n",
    "\n",
    "Только позже я нашел в себе интерес читать романы и некоторые из них мне нравились. Или я просто нашел романы и сюжетные книги, которые мне понравились.\n",
    "\n",
    "Самое главное, что я понял так это то что никто никогда не сможет заставить вас насильно почитать что-то пока это вам самим не станет интересно или нужно. C книгами, как и с фильмами — трудно заставить себя смотреть черно белое кино если у тебя нет никакого интереса к нему.\n",
    "\n",
    "И если книга вам не нравится никогда не стоит боятся бросить её на середине. Глупо дочитывать не интересную книгу до конца только для галочки.\n",
    "TL;DR\n",
    "\n",
    "\"\"\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " В детстве ненавидел художественную литературу. Все книги в которых было хоть что-то с малейшим намеком на сюжет с главными героями — шли мимо. Особенно это касается русской литературы в которой сначала ты читаешь сказки про жадную старуху или хитрую лису и не очень хитрого колобка, а к 10 классу ты уже разбираешь депрессивные книги в которых кто-то да точно умрет.\n",
      "\n",
      "Это не значит, что я ненавидел вообще книги, просто я читал другие. Мне нравился научный сленг и естественно я зачитывался детскими энциклопедиями. Такими чтоб картинок побольше было и текст поинтересней и обязательно про космоc. Планеты, полёты в космос, черные дыры это лайк. В то время я хотел стать астрономом.\n",
      "\n",
      "Что касается книг про динозавров для меня это было ерундой, пусть динозавры сами про себя и читают, они не имеют ничего общего с настоящим миром.\n",
      "\n",
      "Только позже я нашел в себе интерес читать романы и некоторые из них мне нравились. Или я просто нашел романы и сюжетные книги, которые мне понравились.\n",
      "\n",
      "Самое главное, что я понял так это то что никто никогда не сможет заставить вас насильно почитать что-то пока это вам самим не станет интересно или нужно. C книгами, как и с фильмами — трудно заставить себя смотреть черно белое кино если у тебя нет никакого интереса к нему.\n",
      "\n",
      "И если книга вам не нравится никогда не стоит боятся бросить её на середине. Глупо дочитывать не интересную книгу до конца только для галочки.\n",
      "TL;DR\n",
      "\n",
      "— Дед, что ты мне рассказываешь, ты ведь не был в музее? А где там экспонаты хранятся? — спросил я, когда мы возвращались из музея.\n",
      "\n",
      "— А вот и нет. — Дед засмеялся и махнул рукой. — Это тебе не музей. Тут все хранится на складе.\n",
      "\n",
      "Я подошел к одному из контейнеров и начал копаться в нем, рассматривая всякие штуки и стараясь найти что-нибудь интересное. Я искал что-нибудь необычное, поэтому не сразу заметил деда, стоявшего рядом.\n",
      "\n",
      "— Ну ты даешь. — Дед смотрел на меня улыбаясь и хлопая меня по плечу. — И зачем тебе это надо?\n",
      "\n",
      "— А зачем мне надо? — спросил я, глядя ему в глаза.\n",
      "\n",
      "— Это я тебе должен сказать. — Дед похлопал меня по плечу.\n",
      "\n",
      "— Я хочу, чтобы ты мне помог.\n"
     ]
    }
   ],
   "source": [
    "# Пример вероятностного сэмплирвоания с ограничением\n",
    "out = model.generate(input_ids, \n",
    "                     do_sample=True,\n",
    "                     temperature=1.3,\n",
    "                     top_k=20,\n",
    "                     top_p=0.8,\n",
    "                     max_length=500,\n",
    "                    )\n",
    "# Декодирование токенов\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61c6b52aa7a193b32d945311ac88619776407f985ed5a58e3470b057623c4e5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
